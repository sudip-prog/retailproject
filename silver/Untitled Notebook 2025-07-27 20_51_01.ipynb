{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e21c55bd-b8a6-4c18-a0ff-445837758abf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "df = spark.read.format(\"delta\").load(\"abfss://retailstream360@storageaccountsudip1.dfs.core.windows.net/bronze/customers/\")\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93029c17-09d8-40c8-9264-85413cccb51c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63927dd4-502c-44db-9a60-e89395584775",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql import SparkSession\n",
    "from utils.logger import get_logger\n",
    "from utils.audit_logger import log_audit\n",
    "from utils.schema_validator import validate_schema\n",
    "from utils.scd1 import apply_scd1\n",
    "from utils.data_quality import run_data_quality_checks\n",
    "from utils.schema_metadata import silver_table_metadata\n",
    "\n",
    "\n",
    "logger = get_logger(\"customers_silver\")\n",
    "table_conf = silver_table_metadata[\"customers\"]\n",
    "source_path = table_conf[\"source_path\"]\n",
    "target_path = table_conf[\"target_path\"]\n",
    "schema = table_conf[\"schema\"]\n",
    "primary_keys = table_conf[\"primary_keys\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39c54436-8d2b-46a5-b837-09c5c6d63be9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "df = spark.read.format(\"delta\").load(source_path)\n",
    "logger.info(\"Loaded customers data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb7cbe45-bb89-44b0-b478-f6d19a4cb55b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python \n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8223e083-5fc5-4a1a-a09e-9f6fb49dad4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "if not validate_schema(df, schema):\n",
    "    log_audit(spark, \"customers\", \"failed\", \"Schema validation failed\")\n",
    "    print(\"Schema validation failed\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b04cd191-29ff-4d0a-9474-1d64bcfaebd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "logger.info(\"Running data quality checks\")\n",
    "dq_metrics = run_data_quality_checks(df, primary_keys)\n",
    "logger.info(\"Data Quality checks done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f17e7b6c-ac2f-47e9-8ded-541f61a6a776",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "logger.info(\"Applying SCD1 Logic\")\n",
    "apply_scd1(spark, df, target_path, primary_keys)\n",
    "log_audit(spark, \"customers\", \"success\", \"Loaded with SCD1\", dq_metrics.get(\"row_count\", 0))\n",
    "logger.info(\"SCD1 checked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88251c6c-d7b4-46b6-9c02-a8d3f10a347b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "=== Confluent Cloud API key ===\n",
    "\n",
    "API key:\n",
    "YN4I3CHDNNVPHGIY\n",
    "\n",
    "API secret:\n",
    "V2B2pBI8IzbmX6S2YQZiKBaeaXFqSXtC484isML2vMgia264QD7nmhD2vEDCzVKH\n",
    "\n",
    "Resource:\n",
    "lkc-x8d1pk\n",
    "\n",
    "Bootstrap server:\n",
    "pkc-56d1g.eastus.azure.confluent.cloud:9092\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f98dd38-31f6-4a1e-8f2b-bd4c52b8c034",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StringType, StructType\n",
    "\n",
    "# Create Spark session with Delta and Kafka support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Kafka to Delta\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Kafka topic and schema setup\n",
    "kafka_bootstrap_servers = \"<your-confluent-bootstrap-server>:9092\"\n",
    "kafka_topic = \"<your-topic>\"\n",
    "kafka_api_key = \"<your-api-key>\"\n",
    "kafka_api_secret = \"<your-api-secret>\"\n",
    "\n",
    "# Build SASL_SSL options for Confluent Cloud\n",
    "kafka_options = {\n",
    "    \"kafka.bootstrap.servers\": kafka_bootstrap_servers,\n",
    "    \"subscribe\": kafka_topic,\n",
    "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
    "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "    \"kafka.sasl.jaas.config\": f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"apikey\" password=\"{kafka_api_secret}\";',\n",
    "    \"startingOffsets\": \"latest\"\n",
    "}\n",
    "\n",
    "# Define your schema based on expected Kafka message format\n",
    "message_schema = StructType().add(\"id\", StringType()).add(\"value\", StringType())\n",
    "\n",
    "# Read data from Kafka\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .options(**kafka_options) \\\n",
    "    .load()\n",
    "\n",
    "# Deserialize JSON Kafka value\n",
    "parsed_df = df.selectExpr(\"CAST(value AS STRING) as json\") \\\n",
    "    .select(from_json(col(\"json\"), message_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# Write to Delta format\n",
    "query = parsed_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/delta/checkpoints/kafka_stream\") \\\n",
    "    .start(\"/delta/output/kafka_data\")\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c61e1767-3f07-4410-8749-73cba009a669",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "from pyspark.sql.functions import col, from_json\n",
    "\n",
    "KAFKA_BROKER = \"pkc-56d1g.eastus.azure.confluent.cloud:9092\"\n",
    "KAFKA_TOPIC = \"orders\"\n",
    "checkpoint_path = \"abfss://retailstream360@storageaccountsudip1.dfs.core.windows.net/silver/checkpoints\"\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, StringType, DateType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"order_id\", StringType()),\n",
    "    StructField(\"customer_id\", StringType()),\n",
    "    StructField(\"product_id\", StringType()), \n",
    "    StructField(\"order_date\", DateType()),\n",
    "    StructField(\"status\", StringType())])\n",
    "\n",
    "\n",
    "# Read from Kafka\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\",KAFKA_BROKER) \\\n",
    "      .option(\"subscribe\", KAFKA_TOPIC) \\\n",
    "      .option(\"startingOffsets\", \"earliest\") \\\n",
    "      .option(\"kafka.security.protocol\",\"SASL_SSL\") \\\n",
    "      .option(\"kafka.sasl.mechanism\", \"PLAIN\") \\\n",
    "      .option(\"kafka.sasl.jaas.config\", \"\"\"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"YN4I3CHDNNVPHGIY\" password=\"V2B2pBI8IzbmX6S2YQZiKBaeaXFqSXtC484isML2vMgia264QD7nmhD2vEDCzVKH\";\"\"\") \\\n",
    "    .load()\n",
    "\n",
    "# Deserialize and process (example for JSON data)\n",
    "processed_df = df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# processed_df.writeStream.format(\"console\").option(\"truncate\",True).start()\n",
    "# #query.awaitTermination()\n",
    "\n",
    "# Write to Delta Lake\n",
    "# query = processed_df.writeStream \\\n",
    "#     .format(\"delta\") \\\n",
    "#     .option(\"kafka.bootstrap.servers\", \"pkc-56d1g.eastus.azure.confluent.cloud:9092\")\\\n",
    "#     .option(\"topic\", \"orders\")\\\n",
    "#     .option(\"kafka.security.protocol\", \"SASL_SSL\")\\\n",
    "#     .option(\"kafka.sasl.mechanism\", \"PLAIN\")\\\n",
    "#     .option(\"checkpointLocation\", \"abfss://retailstream360@storageaccountsudip1.dfs.core.windows.net/silver/checkpoints\") \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .option(\"kafka.sasl.jaas.config\",'kafkashadedorg.apache.kafka.common.security.plain.PlainLoginModule required username=\"YN4I3CHDNNVPHGIY\" password=\"V2B2pBI8IzbmX6S2YQZiKBaeaXFqSXtC484isML2vMgia264QD7nmhD2vEDCzVKH\";') \\\n",
    "#     .toTable(\"retail_catalog.silver.orders\")\n",
    "\n",
    "query = processed_df.writeStream \\\n",
    "  .format(\"delta\")\\\n",
    "  .outputMode(\"append\")\\\n",
    "  .option(\"kafka.bootstrap.servers\", KAFKA_BROKER) \\\n",
    "  .option(\"topic\", KAFKA_TOPIC) \\\n",
    "  .option(\"kafka.security.protocol\",\"SASL_SSL\") \\\n",
    "  .option(\"kafka.sasl.mechanism\", \"PLAIN\") \\\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\\\n",
    "  .option(\"kafka.sasl.jaas.config\", \"\"\"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"EFDJSWUZVCTJ4TZI\" password=\"pyyiWLLrgySmD+8CA+CKaT4TMIBNPzf+hj9N43mlNezeNYdMX/9qPdmn8FAqHzjb\";\"\"\") \\\n",
    "  .option(\"mergeSchema\",True)\\\n",
    "  .toTable(\"retail_catalog.silver.orders\")\n",
    "\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b55f575-9491-4718-8218-9d45949ace7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import col, from_json\n",
    "\n",
    "KAFKA_BROKER = \"pkc-56d1g.eastus.azure.confluent.cloud:9092\"\n",
    "KAFKA_TOPIC = \"orders\"\n",
    "checkpoint_path = \"abfss://retailstream360@storageaccountsudip1.dfs.core.windows.net/silver/checkpoints\"\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, StringType, DateType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"order_id\", StringType()),\n",
    "    StructField(\"customer_id\", StringType()),\n",
    "    StructField(\"product_id\", StringType()), \n",
    "    StructField(\"order_date\", DateType()),\n",
    "    StructField(\"status\", StringType())])\n",
    "kafka_options = {\n",
    "    \"kafka.bootstrap.servers\" : KAFKA_BROKER, \n",
    "    \"topic\": KAFKA_TOPIC, \n",
    "    \"subscribe\": KAFKA_TOPIC,\n",
    "    \"kafka.security.protocol\":\"SASL_SSL\",\n",
    "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "    \"checkpointLocation\": checkpoint_path, \n",
    "    \"kafka.sasl.jaas.config\":\"\"\"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"EFDJSWUZVCTJ4TZI\" password=\"pyyiWLLrgySmD+8CA+CKaT4TMIBNPzf+hj9N43mlNezeNYdMX/9qPdmn8FAqHzjb\";\"\"\",\n",
    "    \"mergeSchema\": True,\n",
    "    \"startingOffsets\" : \"earliest\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "566c6d14-d988-4d02-8d8a-75c99365348c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "# Read from Kafka\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .options(**kafka_options) \\\n",
    "    .load()\n",
    "\n",
    "# Deserialize and process (example for JSON data)\n",
    "processed_df = df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "\n",
    "\n",
    "query = processed_df.writeStream \\\n",
    "  .format(\"delta\")\\\n",
    "  .outputMode(\"append\")\\\n",
    "  .options(**kafka_options)\\\n",
    "  .trigger(once=True)\\\n",
    "  .toTable(\"retail_catalog.silver.orders\")\n",
    "\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cc315d6-c54e-4af3-ae04-7500c9059549",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2626cd20-bab6-4429-9440-3f451615c9d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * from retail_catalog.silver.orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "207cf62f-7018-4059-af77-82ff895a603c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE SCHEMA IF NOT EXISTS retail_catalog.silver;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS retail_catalog.silver.orders\n",
    "USING DELTA\n",
    "LOCATION 'abfss://retailstream360@storageaccountsudip1.dfs.core.windows.net/silver/orders';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43a2d7ca-e829-49c3-8a2e-2d372c238143",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE TABLE IF NOT EXISTS retail_catalog.silver.transactions\n",
    "USING DELTA\n",
    "LOCATION 'abfss://retailstream360@storageaccountsudip1.dfs.core.windows.net/silver/transactions';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ca51ecf-bf3e-4ab1-a050-4b24967d57b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "from pyspark.sql.functions import col, from_json\n",
    "\n",
    "KAFKA_BROKER = \"pkc-56d1g.eastus.azure.confluent.cloud:9092\"\n",
    "KAFKA_TOPIC = \"transactions\"\n",
    "checkpoint_path = \"abfss://retailstream360@storageaccountsudip1.dfs.core.windows.net/silver/checkpoints_transactions\"\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, StringType, DateType\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType()),\n",
    "    StructField(\"order_id\", StringType()),\n",
    "    StructField(\"product_id\", StringType()), \n",
    "    StructField(\"quantity\", IntegerType()),\n",
    "    StructField(\"price\", DoubleType()),\n",
    "    StructField(\"transaction_date\", DateType())])\n",
    "\n",
    "kafka_options = {\n",
    "    \"kafka.bootstrap.servers\" : KAFKA_BROKER, \n",
    "    \"topic\": KAFKA_TOPIC, \n",
    "    \"subscribe\": KAFKA_TOPIC,\n",
    "    \"kafka.security.protocol\":\"SASL_SSL\",\n",
    "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "    \"checkpointLocation\": checkpoint_path, \n",
    "    \"kafka.sasl.jaas.config\":\"\"\"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"EFDJSWUZVCTJ4TZI\" password=\"pyyiWLLrgySmD+8CA+CKaT4TMIBNPzf+hj9N43mlNezeNYdMX/9qPdmn8FAqHzjb\";\"\"\",\n",
    "    \"mergeSchema\": True, \n",
    "    \"startingOffsets\" : \"earliest\"\n",
    "}\n",
    "# Read from Kafka\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .options(**kafka_options) \\\n",
    "    .load()\n",
    "\n",
    "# Deserialize and process (example for JSON data)\n",
    "processed_df = df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "\n",
    "\n",
    "query = processed_df.writeStream \\\n",
    "  .format(\"delta\")\\\n",
    "  .outputMode(\"append\")\\\n",
    "  .option(\"checkpointLocation\",checkpoint_path)\\\n",
    "  .trigger(availableNow=True)\\\n",
    "  .toTable(\"retail_catalog.silver.transactions\")\n",
    "\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b0611b9-bf50-4020-ba5d-e8410e568b9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Select * from retail_catalog.silver.transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "63bc5498-04f9-4e54-97cc-0b2968b93c3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "kafka_options = {\n",
    "    # Kafka bootstrap servers (Confluent Cloud or on-prem)\n",
    "    \"kafka.bootstrap.servers\": \"pkc-56d1g.eastus.azure.confluent.cloud:9092\",  # e.g. \"pkc-xxxxx.us-central1.gcp.confluent.cloud:9092\"\n",
    "\n",
    "    # Topic to subscribe to\n",
    "    \"subscribe\": \"orders\",\n",
    "\n",
    "    # Starting offset (earliest, latest, or specific)\n",
    "    \"startingOffsets\": \"earliest\",\n",
    "\n",
    "    # Security protocol\n",
    "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
    "\n",
    "    # Authentication mechanism\n",
    "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "\n",
    "    # SASL username and password (typically API key and secret from Confluent Cloud)\n",
    "    \"kafka.sasl.jaas.config\": \"org.apache.kafka.common.security.plain.PlainLoginModule required username='YN4I3CHDNNVPHGIY' password='V2B2pBI8IzbmX6S2YQZiKBaeaXFqSXtC484isML2vMgia264QD7nmhD2vEDCzVKH';\"\n",
    "}\n",
    "\n",
    "# Read from Kafka\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .options(**kafka_options) \\\n",
    "    .load()\n",
    "\n",
    "# Deserialize and process (example for JSON data)\n",
    "processed_df = df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# Write to Delta Lake\n",
    "query = processed_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/path/to/checkpoint/location\") \\\n",
    "    .toTable(\"your_delta_table_name\")\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "    )\n",
    "}\n",
    "\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .options(**kafka_options) \\\n",
    "    .load()\n",
    "\n",
    "df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9274b9f0-db3c-4503-9bfe-5174cc57c800",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "  \"order_id\": \"O000004\",\n",
    "  \"customer_id\": \"C0001\",\n",
    "  \"product_id\": \"P0001\",\n",
    "  \"order_date\": \"2025-07-17\",\n",
    "  \"status\": \"DELIVERED\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aefdc5bc-7176-43f7-95bb-e09fe1e175e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "  \"transaction_id\": \"T000016\",\n",
    "  \"order_id\": \"O000016\",\n",
    "  \"product_id\": \"P0002\",\n",
    "  \"quantity\": 5,\n",
    "  \"price\": 740.78,\n",
    "  \"transaction_date\": \"2025-03-10\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56e0cb2b-0c07-4ffe-9cd1-413d4d53b7f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql import SparkSession\n",
    "from utils.logger import get_logger\n",
    "from utils.audit_logger import log_audit\n",
    "from utils.schema_validator import validate_schema\n",
    "from utils.data_quality import run_data_quality_checks\n",
    "from delta.tables import DeltaTable\n",
    "from utils.schema_metadata import silver_table_metadata\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, StringType, DateType\n",
    "\n",
    "\n",
    "KAFKA_BROKER = \"pkc-56d1g.eastus.azure.confluent.cloud:9092\"\n",
    "KAFKA_TOPIC = \"orders\"\n",
    "checkpoint_path = \"abfss://retailstream360@storageaccountsudip1.dfs.core.windows.net/silver/checkpoints\"\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, StringType, DateType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"order_id\", StringType()),\n",
    "    StructField(\"customer_id\", StringType()),\n",
    "    StructField(\"product_id\", StringType()), \n",
    "    StructField(\"order_date\", DateType()),\n",
    "    StructField(\"status\", StringType())])\n",
    "kafka_options = {\n",
    "    \"kafka.bootstrap.servers\" : KAFKA_BROKER, \n",
    "    \"topic\": KAFKA_TOPIC, \n",
    "    \"subscribe\": KAFKA_TOPIC,\n",
    "    \"kafka.security.protocol\":\"SASL_SSL\",\n",
    "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "    \"checkpointLocation\": checkpoint_path, \n",
    "    \"kafka.sasl.jaas.config\":\"\"\"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"EFDJSWUZVCTJ4TZI\" password=\"pyyiWLLrgySmD+8CA+CKaT4TMIBNPzf+hj9N43mlNezeNYdMX/9qPdmn8FAqHzjb\";\"\"\",\n",
    "    \"mergeSchema\": True,\n",
    "    \"startingOffsets\" : \"earliest\"\n",
    "}\n",
    "\n",
    "# Read from Kafka\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .options(**kafka_options) \\\n",
    "    .load()\n",
    "\n",
    "# Deserialize and process (example for JSON data)\n",
    "processed_df = df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce7420a2-fc92-434f-bc6d-cb3ce608fb77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "processed_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82c262ab-d6c3-4e86-bf07-71f79b18893a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "if not validate_schema(processed_df, schema):\n",
    "    print(\"Schema Validation Failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2350d0df-cba7-495e-8c04-221b639f2639",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "table_conf = silver_table_metadata[\"orders\"]\n",
    "primary_keys = table_conf[\"primary_keys\"]\n",
    "dq_metrics = run_data_quality_checks(query, primary_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "879482a7-c742-4a2a-876a-0bcd7e7af326",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "query = processed_df.writeStream \\\n",
    "  .format(\"delta\")\\\n",
    "  .outputMode(\"append\")\\\n",
    "  .options(**kafka_options)\\\n",
    "  .trigger(once=True)\\\n",
    "  .toTable(\"retail_catalog.silver.orders\")\n",
    "\n",
    "\n",
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-07-27 20_51_01",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
